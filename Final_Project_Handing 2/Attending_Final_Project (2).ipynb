{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Project\n",
    "\n",
    "The final project is designed to let you apply what you have learned so far, and demonstrate that you have mastered it. The submission will be graded on the correctness and performance of the execution of your analysis (50%), the ambitiousness of the problems chosen (30%), and the creativity of your questions and solutions (20%).\n",
    "\n",
    "Your submission should include all outputs and be *self-contained*, so it can be executed if necessary.\n",
    "\n",
    "The submission includes two parts:\n",
    "1. this notebook\n",
    "2. a 15-min presentation, to be held on May 7\n",
    "\n",
    "\n",
    "## Submission\n",
    "The project is due on ***May 06, 23:59 CET*** (counted as the time stamp when it is received). Late submissions will **not** be considered, and graded as 0! \n",
    "\n",
    "To submit, please:\n",
    "\n",
    "1. copy this file and all additional data into a folder with your group ID\n",
    "3. zip the folder\n",
    "4. send a copy of the zip file to Dirk Hovy <dirk.hovy@unibocconi.it> and Tommaso Fornaciari <fornaciari@unibocconi.it>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Research Question(s) (2 pts)\n",
    "\n",
    "Describe what question you are investigating with the data (max. 100 words)\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data, Preprocessing, Annotation, and Analysis (6 pts)\n",
    "\n",
    "Find a data set for **text classification** and a data set for **structured prediction**. These can be the same.\n",
    "Kaggle is a good place to start, or the Google data set search. \n",
    "\n",
    "The data sets should have **at least 5,000** documents each. **At least 2000 instances** need to be labeled. \n",
    "\n",
    "If there is no label available, you can annotate your own and get up to **2 bonus points**, depending on the amount and complexity of the annotation.\n",
    "\n",
    "Split each data set into dedicated training, development, and test sets (if they do not include these already)!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Briefly (max. 100 words!) describe the content and type of the data set, and what you are planning to look at. \n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess the data and explain (max. 200 words) which preprocessing steps you chose and why, and give statistics of the number of documents, types, and tokens, before and after preprocessing.\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Provide some basic analysis of the training data with any of the following analysis methods (justify your choices):\n",
    "\n",
    "1. Topic modeling with LDA. Justify your choice of number of topics!\n",
    "2. Word embeddings \n",
    "3. Document embeddings: visualize these and show a clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Prediction (17 pts)\n",
    "\n",
    "\n",
    "### 4.1 Classification (9 pts)\n",
    "Build a predictive model of the target label and use appropriate performance metrics. Your predictive analysis needs to involve **all** of the following, summarized in a table:\n",
    "\n",
    "1. a most-frequent-label baseline (1 pt)\n",
    "2. a `LogisticRegression()` baseline with default parameters and 2-6 gram character TFIDF features (1 pt)\n",
    "3. **at least** two more predictive models, including description/justification of the optmization steps taken (6 pts).\n",
    "4. bootstrap sampling significance tests of the performance difference between your best model and each of the two baselines (1 pts)\n",
    "\n",
    "NB: Do make sure that the optimization steps are done on the development split and do *not* include the test split! Training on the test set will be graded 0!\n",
    "\n",
    "\n",
    "### 4.1 Structured Prediction (8pts)\n",
    "Adapt the Structured Perceptron to your sequence prediction task, and note the performance as baseline (3 pt).\n",
    "Implement a suitable neural net architecture (in `keras`) on the data (4 pts). Compare the best performance of the  models (1 pt)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Visualizations (5 pts)\n",
    "\n",
    "Provide at least 3 visualizations of your work above. These can be in the respective sections. Use labels and legends. Be creative. Just please do not use word clouds..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
